{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lecture: (日本語) Pythonで自然言語処理入門ー東京大学Google社員特別講義.ipynb のコピー","provenance":[{"file_id":"1NakptCPc6yUj5B04EWj-myQApdEneJ9A","timestamp":1603698242004},{"file_id":"1t9rTBxLX-7qwNLQaPBItU5FQto76VGgO","timestamp":1572249469063},{"file_id":"1gHIINJF0sk1IJ5d-abAZSbMmCpXEOKJM","timestamp":1571997665424},{"file_id":"1ami3r0VtZdsZlxtdNTaKkdzq3X3wp16h","timestamp":1571812373173},{"file_id":"1s2mqY-3qlXoWLNoaCyepHLaMCZy5FTl3","timestamp":1571214148084},{"file_id":"13fNpdUz0AD5j4fwQtk_lnZaQn-dRAFp6","timestamp":1570591383413},{"file_id":"1yobcsNmBpJqYZzM0kRcN2N0aialXDPYa","timestamp":1570442659156}],"collapsed_sections":[],"last_runtime":{"build_target":"//quality/ranklab/experimental/notebook:rl_colab","kind":"private"}},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9CMQbe-o58-X"},"source":["# Pythonで自然言語処理入門 (日本語) "]},{"cell_type":"markdown","metadata":{"id":"N20xcYwLAelL"},"source":["\n","この講義では以下のようなNLP(自然言語処理)の基本的な概念を紹介します:\n","\n","*   単語分割 (Tokenization)\n","*   形態素解析 (Syntactic Analysis/Part-of-speech tagging)\n","*   単語出現頻度 (Word Frequencies)\n","*   単語の重要度：TF-IDF値 (Term Frequency - Inverse Document Frequency)\n","*   ドキュメント間の類似度 (Document Similarity)\n","\n","\n","また、次のような実践的な内容に対する演習の行います:\n","\n","*   日本語Wikipediaの記事に対する処理\n","*   処理した内容の可視化\n","*   ある記事内の最も関連性の高いキーワードの算出\n","*   ウィキペディア記事(ドキュメント)間の類似度"]},{"cell_type":"markdown","metadata":{"id":"20EIocXkDcqc"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"TQ4zccw7p3v0"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"J2g3iqEKCruW"},"source":["まず、このnotebookを実行するにあたって必要になるツールのインストールとライブラリのインポートを行いましょう。"]},{"cell_type":"code","metadata":{"id":"RMLRmM0djMPh","cellView":"form","executionInfo":{"status":"ok","timestamp":1603700819134,"user_tz":-540,"elapsed":7192,"user":{"displayName":"Reon Akiyama","photoUrl":"","userId":"04074901317082462028"}},"outputId":"d797d641-83d3-495f-8f02-1fb0ed7e11b7","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["#@title **[実行必要]** ツールのインストール\n","# ツールのインストール\n","# Mecabをインストール\n","!pip install mecab-python3\n","# ワードクラウド可視化をインスtーる\n","!pip install wordcloud"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/6d/807491c94bd78c2641d0245155f3b40c000dc49757ec1d249080218661bc/mecab_python3-1.0.2-cp36-cp36m-manylinux2010_x86_64.whl (3.5MB)\n","\u001b[K     |████████████████████████████████| 3.5MB 4.2MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.2\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.0.0)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dEzMCx6qOiKd","cellView":"form","executionInfo":{"status":"ok","timestamp":1603700830076,"user_tz":-540,"elapsed":616,"user":{"displayName":"Reon Akiyama","photoUrl":"","userId":"04074901317082462028"}}},"source":["#@title **[実行必要]** ライブラリーのインポート\n","# ライブラリーのインポート\n","import MeCab\n","from collections import Counter\n","import codecs\n","import nltk\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as font_manager\n","from math import log\n","import re\n","import sqlite3\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bPqFqScjSB_","cellView":"form","executionInfo":{"status":"ok","timestamp":1603700835667,"user_tz":-540,"elapsed":4557,"user":{"displayName":"Reon Akiyama","photoUrl":"","userId":"04074901317082462028"}},"outputId":"13754b21-0125-4eda-dafc-63b3703b9f46","colab":{"base_uri":"https://localhost:8080/","height":544}},"source":["#@title **[実行必要]** 日本語フォントのインストール\n","# Please execute this cell to download the necessary japanese fonts for visualization.\n","# Download the necessary fonts for adding japanese words in graphs:\n","!rm -r IPAexfont*\n","!wget https://oscdl.ipa.go.jp/IPAexfont/IPAexfont00301.zip\n","!unzip IPAexfont00301.zip\n","# Download the necessary fonts to make the word cloud in japanese \n","!rm NotoSans*\n","!rm README\n","!rm LICENSE_*\n","!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip\n","!unzip NotoSansCJKjp-hinted.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'IPAexfont*': No such file or directory\n","--2020-10-26 08:27:11--  https://oscdl.ipa.go.jp/IPAexfont/IPAexfont00301.zip\n","Resolving oscdl.ipa.go.jp (oscdl.ipa.go.jp)... failed: Name or service not known.\n","wget: unable to resolve host address ‘oscdl.ipa.go.jp’\n","unzip:  cannot find or open IPAexfont00301.zip, IPAexfont00301.zip.zip or IPAexfont00301.zip.ZIP.\n","rm: cannot remove 'NotoSans*': No such file or directory\n","rm: cannot remove 'README': No such file or directory\n","rm: cannot remove 'LICENSE_*': No such file or directory\n","--2020-10-26 08:27:12--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip\n","Resolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 74.125.20.128, 2607:f8b0:400e:c07::80\n","Connecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|74.125.20.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 121096772 (115M) [application/zip]\n","Saving to: ‘NotoSansCJKjp-hinted.zip’\n","\n","NotoSansCJKjp-hinte 100%[===================>] 115.49M   131MB/s    in 0.9s    \n","\n","2020-10-26 08:27:13 (131 MB/s) - ‘NotoSansCJKjp-hinted.zip’ saved [121096772/121096772]\n","\n","Archive:  NotoSansCJKjp-hinted.zip\n","  inflating: LICENSE_OFL.txt         \n","  inflating: NotoSansCJKjp-Black.otf  \n","  inflating: NotoSansCJKjp-Bold.otf  \n","  inflating: NotoSansCJKjp-DemiLight.otf  \n","  inflating: NotoSansCJKjp-Light.otf  \n","  inflating: NotoSansCJKjp-Medium.otf  \n","  inflating: NotoSansCJKjp-Regular.otf  \n","  inflating: NotoSansCJKjp-Thin.otf  \n","  inflating: NotoSansMonoCJKjp-Bold.otf  \n","  inflating: NotoSansMonoCJKjp-Regular.otf  \n","  inflating: README                  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rR9F-GRdDgVU"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"5Uo3P20tDmfq"},"source":["### NLP (Natural Language Processing/自然言語処理)\n","\n","自然言語処理(NLP)とは、自然言語の形をした情報を処理する・生成するための手法を研究するコンピュータサイエンスの分野です。\n","言い換えると、NLPの目的はプログラムに人間の言葉で表された情報を理解させることになります。\n","NLPは学際的な分野であり、文字列を効率的に処理するための数学的なアルゴリズムから、商品レビューの中に書かれた皮肉というような言語学的に複雑な事象を扱うためのモデルまで、扱う対象は多岐に渡ります。\n","\n","この講義では、NLPを使って実践的で面白い結果を得るために必要な基本的な概念を紹介し、次のような処理を行うプログラムをみていきます: \n","\n","*   テキストからその構成要素(=単語)への分割\n","*   単語の分布を観察\n","*   文書中の関連性の高い単語の簡潔な表現\n","*   その表現に基づいた文書間の類似度の計算"]},{"cell_type":"markdown","metadata":{"id":"eTTP0dgvKLx5"},"source":["### 単語分割 / トークン化 (Tokenization)\n","\n","NLPにおいて、最初にしなければならないタスクは解析したいテキストの分割(tokenization)です。\n","\n","トークナイゼーションとは、与えられたテキストを \"トークン\" と呼ばれる小さい部分に分けることです。\n","\n","通常、これはテキストを単語トークンに分割することを意味しますが、解析したい内容によっては文や形態素(morphemes)などのような他の単位でトークナイゼーションすることもあります。\n","もしmultiword tokenを使う場合は、この処理を\"chunking\"と呼ぶこともあります。\n","\n","\n","Tokenizationの目的は、文書中の要素とその関係をコンピュータに区別させることです。この単語分割の処理によって、同じ単語は何回現れたか（出現頻度）などを計算することが可能になり、そういう計算から得られる情報量を他のNLPタスク(形態素解析、意味解析、感情解析、など)の為に使えます。\n","\n","\n","単語とは一体何なのかということについては、言語学の分野においても議論の余地があります。特にこれは、日本語を扱う際に重要になってきます。\n","\n","例えば「東京大学」は一つの単語なのでしょうか？それとも(東京 + 大学)で二つの単語なのでしょうか？\n","\n","また「食べさせられる」は1つの単語として扱うべきでしょうか？\n","\n","それとも ```食べ + させ + られる``` と分割するべきでしょうか？\n","\n","これはとても難しい問題で、この定義は解決したいNLP問題に依存します。"]},{"cell_type":"markdown","metadata":{"id":"qp46GTGFPDUf"},"source":["### 正規化 (Normalization)、語幹抽出 (Stemming)と見出し語変換 (Lemmatization)\n","\n","自然言語とその表記体系の特徴として、不規則であることと、同じ概念を異なる方法で表現することが可能であるということが挙げられます。\n","\n","この曖昧さにはいくつかのレベルがあります:\n","\n","単にスペリングの違いによるものもあります。(省略, 大文字/小文字):\n","\n","```\n","monday, Monday, Mon., mon.\n","```\n","\n","日本の数の表記法のように、より複雑なバリエーションもありえます:\n","\n","```\n","2,２, に, 二, 二つ, ２個、２台、２枚、...\n","```\n","\n","これらの曖昧さはデータセットを疎にしてしまい、有益な情報を取り出すためのパターンを見つけるのを難しくしてしまいます。\n","そのため、解析したい内容次第ではあるものの、こういった表記ゆれはすべて同じトークンだとみなしたい場合が多いです。\n","また、すべてを \"\\<NUMBER\\>\"　というタグで置き換えてしまうのも良いでしょう。\n","\n","このような、すべての表記ゆれの後を 一つの統一された形のトークンやタグに置き換えることを正規化(Normalization)といいます。\n","これは表記の違いを抽象化して、データセットにまたがるより一般的なパターンを見つけるのに役立ちます。\n","\n","**例:**\n","\n","**正規化前**\n","\n","```\n","パンを六つ買いました\n","店を出た時にひとつ落ちました。\n","そのあと２個食べました。\n","３つ余りました。\n","```\n","\n","**正規化後**\n","\n","```\n","パンを<NUMBER><VERB>\n","店を出た時に<NUMBER><VERB>\n","そのあと<NUMBER><VERB>\n","<NUMBER><VERB>。\n","```\n","\n","また考慮するべきこととして、同じ単語や概念が文中での役割に応じて異なる形で現れるということが挙げられます。\n","これは特に活用や後置詞のあるような言語(morphologically rich language)を扱うときに重要です。\n","\n","日本語における例としては動詞が挙げられます。\n","もし、テキストに含まれる単語の傾向を知るために、出現頻度を数えるのだとしたら、次のような単語を異なる単語とは数えたくないでしょう:\n","\n"," ```\n"," 食べる, 食べられる、食べさせる、食べさせられる、食べた、食べましょう、食べている、食べていた...\n"," ```\n","\n","正規化の処理には、スペリングの正規化(Canonicalization)やタグに置き換えるといったいくつかの手法が存在します。\n","\n","しかし、もっとも一般的で実装しやすいのは、語幹抽出（Stemming) と 見出し語に変換(Lemmatization)でしょう。\n","\n","語幹抽出(Stemming)とは活用語尾を取り去り、語幹のみを残す処理です。(stem = 幹)\n","上の例ではこのようになります:\n","\n","```\n","食べる, 食べられる、食べさせる、食べさせられる、食べた、...  ==> 食べ\n","```\n","\n","見出し語変換 (Lemmatization) とは活用語尾を置き換えて、*lemmas* (見出し語/辞書に載っている形式の単語) の形にする処理です。\n","上の例ではこのようになります:\n","\n","```\n","食べる, 食べられる、食べさせる、食べさせられる、食べた、... ==> 食べる\n","```"]},{"cell_type":"markdown","metadata":{"id":"jBCttsSuYBW9"},"source":["### 形態素解析 / Part of Speech Tagging (POS)\n","\n","文を単語に分割することができたら、各単語の文中での役割を理解することは有用です。\n","POS (Part of Speech) とは各トークンやトークン列に、文中での文法的な役割を表すタグのことです。そのタグ付けする処理をPOS Taggingと言います。\n","日本語では形態素解析(Syntactic Analysis)とも言います。\n","\n","例えば、次の2つの英文では”run\"という単語がそれぞれ異なる役割で使われているため、それぞれの\"run\"は異なるPOS tagを持ちます:\n","\n","```\n","I [run] everyday.                ===>  動詞\n","This is my first [run]           ===>  名詞\n","```\n","\n","POSは意味解析(Semantic Parsing)の際に否定などの文法的なパターンを検出するのにとても役立ちます。\n","また、固有名詞認識(Named Entity Recognition)や感情解析(Sentiment Analysis)といった他のアルゴリズムを支える基盤になります。"]},{"cell_type":"markdown","metadata":{"id":"wlSjyMsgaApu"},"source":["### MeCabの紹介: 日本語形態素解析ツール (単語分割、POSタグ抽出、見出し語変換)\n","\n","正しく機能するTokenizer、Lemmatizer、Stemmer、Part of Speech Taggerを作るのはとても挑戦的なタスクで、現在も活発に研究されている分野でもあり、いくつかのアプローチがあります:\n","*   機械学習モデルによるタグ付け\n","*   ルールベースタグ付けシステム\n","*   辞書に基づいたタグ付け\n","* など\n","\n","これらをゼロから実用レベルに達するまで自分たちで作るのはとても大変なので、今回は再発明をせずに既存のツールを出来る限り利用するのが良いでしょう。\n","日本語に対するツールで広く使われているデファクトスタンダードのツールとしては、MeCabがあります。\n","\n","Mecabは形態素解析エンジンと呼ばれ、日本語テキストをトークナイゼーションをし、POSタグを与え、見出し語に変換を行います。\n","Mecabにおけるトークンは単語とだいたい等価ですが、より細かい単位にも分割処理を行います。"]},{"cell_type":"markdown","metadata":{"id":"0HpgZcNsdP_n"},"source":["それでは、Mecabの出力結果を表示する関数を定義してみましょう。\n","この関数はあとで再利用できます。"]},{"cell_type":"code","metadata":{"id":"-RYvzwA7KvbO"},"source":["def print_syntatic_analysis(sentence):\n","  tagger = MeCab.Tagger()\n","  text = sentence\n","  node = tagger.parseToNode(text) \n","\n","  while(node):\n","      if node.surface != \"\":\n","          print(node.surface +\"\\t\"+ node.feature)\n","      node = node.next\n","      if node is None:\n","          break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzWJkmaM0toU","executionInfo":{"status":"ok","timestamp":1572270865471,"user_tz":-540,"elapsed":30118,"user":{"displayName":"Juan Ignacio Navarro Horniacek","photoUrl":"","userId":"06671720868110649930"}},"outputId":"0fd4e22f-58c9-444a-c504-cce16a580d33","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print_syntatic_analysis('NLPはとても楽しいです。')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NLP\t名詞,固有名詞,組織,*,*,*,*\n","は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n","とても\t副詞,助詞類接続,*,*,*,*,とても,トテモ,トテモ\n","楽しい\t形容詞,自立,*,*,形容詞・イ段,基本形,楽しい,タノシイ,タノシイ\n","です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n","。\t記号,句点,*,*,*,*,。,。,。\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HzP504zmekUK"},"source":["\"NLPはとても楽しいです。\"という文章に対するMeCabの出力は次のようになります。:\n","\n","```\n","NLP\t名詞,固有名詞,組織,*,*,*,*\n","は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n","とても\t副詞,助詞類接続,*,*,*,*,とても,トテモ,トテモ\n","楽しい\t形容詞,自立,*,*,形容詞・イ段,基本形,楽しい,タノシイ,タノシイ\n","です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n","。\t記号,句点,*,*,*,*,。,。,。\n","```\n","\n","出力の各行は次の8列から構成されます:\n","```\n","{表層形(Surface Form)}, (元の文中での形)\n","{POS},\n","{POS1},\n","{POS2},\n","{POS3},\n","{活用型(Inflection Type)},\n","{活用形(Inflecion Form)},\n","{原形(Lemma)},\n","{読み},\n","{発音}\n","```\n","\n","各行は別々のトークンを表します。\n","\n","```\n","['NLP', 'は', 'とても', '楽しい', 'です', '。']\n","```\n","\n","では次に、もっと活用が多い文で試してみましょう:"]},{"cell_type":"code","metadata":{"id":"OtqnjFck02xO","executionInfo":{"status":"ok","timestamp":1572270865695,"user_tz":-540,"elapsed":30334,"user":{"displayName":"Juan Ignacio Navarro Horniacek","photoUrl":"","userId":"06671720868110649930"}},"outputId":"915bc74b-08ff-4c5a-875f-6bf899f73b89","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["print_syntatic_analysis('食べさせてもらえますか?')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["食べ\t動詞,自立,*,*,一段,未然形,食べる,タベ,タベ\n","させ\t動詞,接尾,*,*,一段,連用形,させる,サセ,サセ\n","て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n","もらえ\t動詞,非自立,*,*,一段,連用形,もらえる,モラエ,モラエ\n","ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n","か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n","?\t名詞,サ変接続,*,*,*,*,*\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4uj7Qoc0gKQm"},"source":["出力は次のようになります:\n","```\n","食べ\t動詞,自立,*,*,一段,未然形,食べる,タベ,タベ\n","させ\t動詞,接尾,*,*,一段,連用形,させる,サセ,サセ\n","て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n","もらえ\t動詞,非自立,*,*,一段,連用形,もらえる,モラエ,モラエ\n","ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n","か\t助詞,副助詞／並立助詞／終助詞,*,*,*,*,か,カ,カ\n","```\n","\n","語が最小の意味をもつ要素にどのように分割されているかみることができます。\n","7番目の列をみると、それぞれの要素の見出し語変換の結果をみることを確認できます:\n","\n","食べる +させる、+て, +もらえる, +ます\n","\n","より複雑な構造をもつ例をみてみましょう:"]},{"cell_type":"code","metadata":{"id":"dO5SXElK3X3q","executionInfo":{"status":"ok","timestamp":1572270865696,"user_tz":-540,"elapsed":30329,"user":{"displayName":"Juan Ignacio Navarro Horniacek","photoUrl":"","userId":"06671720868110649930"}},"outputId":"2ddf5c89-c8a7-48ac-a3fa-42db5f1c9a8b","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["print_syntatic_analysis('食べさせられたくなかったら、手を上げてください。')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["食べ\t動詞,自立,*,*,一段,未然形,食べる,タベ,タベ\n","させ\t動詞,接尾,*,*,一段,未然形,させる,サセ,サセ\n","られ\t動詞,接尾,*,*,一段,連用形,られる,ラレ,ラレ\n","たく\t助動詞,*,*,*,特殊・タイ,連用テ接続,たい,タク,タク\n","なかっ\t助動詞,*,*,*,特殊・ナイ,連用タ接続,ない,ナカッ,ナカッ\n","たら\t助動詞,*,*,*,特殊・タ,仮定形,た,タラ,タラ\n","、\t記号,読点,*,*,*,*,、,、,、\n","手\t名詞,一般,*,*,*,*,手,テ,テ\n","を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n","上げ\t動詞,自立,*,*,一段,連用形,上げる,アゲ,アゲ\n","て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n","ください\t動詞,非自立,*,*,五段・ラ行特殊,命令ｉ,くださる,クダサイ,クダサイ\n","。\t記号,句点,*,*,*,*,。,。,。\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mI3BcA_ZiBGd"},"source":["それでは、これから便利になりますので、与えられた文のトークンから原型と品詞情報を抽出する関数を以下のように定義しましょう。"]},{"cell_type":"code","metadata":{"id":"fMV90xUnOIPI"},"source":["# **[実行必要]**\n","def get_word_lemma_and_pos_info(tagger_node):\n","  # define a function that given a word returns the lemma and pos, pos2 \n","  lemma, pos, pos2 = '_', '_', '_'\n","  if tagger_node:\n","    linguistic_features = tagger_node.feature.split(',')\n","    surface_form = tagger_node.surface\n","    lemma = linguistic_features[6].lower() # keep dictionary form\n","    pos = linguistic_features[0].lower()\n","    pos2 = linguistic_features[1].lower()\n","    if (lemma == '' or lemma == '*') and surface_form:\n","        lemma = surface_form\n","  return lemma, pos, pos2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3levExUfZ5kB"},"source":["# \"ウィキペディアが大好きだよ\"から原型と品詞情報を抽出\n","tagger = MeCab.Tagger()\n","node = tagger.parseToNode(\"ウィキペディアが大好きだよ\")\n","print(get_word_lemma_and_pos_info(node.next))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faDtnu6bhFxt"},"source":["さあここまでで基本的な概念を理解し、MeCabを使えるようになりました。\n","ここからはこれを使って、実際の自然言語のデータセットを使って、NLPでどんなことができるかを見ていきましょう！"]},{"cell_type":"markdown","metadata":{"id":"3eeb89upBCX5"},"source":["# ハンズオン: 日本語Wikipedia\n","\n","---\n","\n","このハンズオンでは、Wikipediaが提供している[日本語版のデータ](https://ja.wikipedia.org/wiki/Wikipedia:%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89)を使います。計算資源と時間の制約から、すべてのデータを用いることはせず、はじめの約10万件の記事を対象に実験を行います。\n","\n","ただ、それでもWikipediaのページからダウンロードし、テキストを抽出するという処理だけ1時間程度かかってしまうので、今回はこちらで事前に抽出したテキストファイルを用意しました。\n","このファイルは以下のコマンドでダウンロードできます:"]},{"cell_type":"code","metadata":{"id":"qct0n0GlDNXY","cellView":"form"},"source":["#@title **[実行必要]** 日本語ウィキペディア一部(50万件の記事)のダンプファイルをダウンロード\n","# Download the text file with the extracted articles.\n","!wget -O japanese_wikipedia_extracted_articles.txt https://storage.googleapis.com/nlp-lecture-data-20191029/japanese_wikipedia_extracted_articles_first_500k.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JbkYRatHEEeO"},"source":["それではダウンロードしてきたファイルの中身をみてみましょう。"]},{"cell_type":"code","metadata":{"id":"BIkAP9G19YYN"},"source":["# Show the first 30 lines of the text file content\n","!head -n 30 japanese_wikipedia_extracted_articles.txt "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzUU0Tk9ERSe"},"source":["ファイルの中身は以下のような構造になっています:\n","```\n","<doc id=\"{記事ID}\" url=\"{記事のURL}\" title=\"{記事タイトル}\">\n","記事の本文\n","</doc>\n","<doc id=\"{記事ID}\" url=\"{記事のURL}\" title=\"{記事タイトル}\">\n","記事の本文\n","</doc>\n","...\n","```\n","\n","記事の本文以外のIDやタイトルといったメタデータはHTMLタグとして、保持されています。"]},{"cell_type":"markdown","metadata":{"id":"bOKOVcD_1nll"},"source":["### 単語出現頻度の計算\n","\n","自然言語のデータセットに対する最も簡単な解析として、統計をとってみるということが挙げられます。\n","ここでは単語出現頻度(Word frequencies)を計算してみましょう。\n","\n","単語出現頻度を計算することはNLPにおいてとても重要です。\n","それ自体多くの事柄を明らかにしますし、多くのNLPのテクニックは単語出現頻度を基にしています。\n","\n","また、単語出現頻度をみることで、単語分割や正規化がうまくできているかを確認することができます。\n","NLPにおいて、正規化と単語出現頻度を行ったり来たりしながら、データ中のノイズを減らすということはよくあります。\n","\n","今回は、まずWikipediaのデータをそのまま解析・可視化するとどうなるかを実験したあとで、正規化等の前処理で結果がどう変わるかをみていきます。"]},{"cell_type":"code","metadata":{"id":"KsXfXonf-meN"},"source":["def count_all_word_frequencies():\n","  all_words = []\n","  tagger = MeCab.Tagger()\n","  with codecs.open(\"japanese_wikipedia_extracted_articles.txt\", \"r\",'utf-8') as file:\n","    for line in file:\n","        node = tagger.parseToNode(line)\n","        while(node):\n","          if node.surface != \"\":\n","            all_words.append(node.surface.lower())\n","          node = node.next\n","          if node is None:\n","              break\n","  return Counter(all_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtKlE-kgav1L"},"source":["all_words = count_all_word_frequencies()\n","# 上位２５単語をプリントしてみましょう。\n","for word in all_words.most_common(25):\n","  print(\"Word: {}, frequency: {}\".format(word[0], word[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYFp5klLJ_E1"},"source":["### 単語出現頻度分布の可視化"]},{"cell_type":"code","metadata":{"id":"pbYCWNpi7bIZ"},"source":["# **[実行必要] **\n","def plot_word_frequency_distribution(word_freq_dict):\n","  # 日本語のフォントを設定する\n","  font_properties = font_manager.FontProperties(fname='IPAexfont00301/ipaexg.ttf')\n","  # プロットのサイズなどを設定する\n","  plt.figure(figsize=(12,5))\n","  plt.xticks(fontsize=13, rotation=90,fontproperties=font_properties)\n","  # 出現回数分布から上位２５単語をプロットする\n","  frequency_distribution = nltk.FreqDist(word_freq_dict)\n","  frequency_distribution.plot(25,cumulative=False)\n","  word_counts = sorted(word_freq_dict.values(), reverse=True)\n","  plt.figure(figsize=(12,5))\n","  plt.loglog(word_counts, linestyle='-', linewidth=1.5)\n","  plt.ylabel(\"Freq\")\n","  plt.xlabel(\"Word Rank\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WE7bDm1Kd_Hk"},"source":["plot_word_frequency_distribution(all_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_DXcNhcKAn5"},"source":["# **[実行必要] **\n","def make_word_cloud(words_freq_dict):\n","  # setup Japanese font for creating the word cloud\n","  font_path = 'NotoSansCJKjp-Light.otf'\n","  # create an image with a Word Cloud with the given word frequencies\n","  wordcloud = WordCloud(width=1500,\n","                        height=1000,\n","                        max_words=900,\n","                        background_color='white',\n","                        colormap='Reds',\n","                        font_path=font_path,\n","                        normalize_plurals=True).generate_from_frequencies(words_freq_dict)\n","  # setup a plot frame without any axis and print the image generated above  \n","  plt.figure(figsize=(17,14))\n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.axis(\"off\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtLk8dESeaSw"},"source":["make_word_cloud(all_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TGIlzebKlxs"},"source":["可視化はできたものの、なんだかよくわからない結果になってしまいました。\n","何が悪かったのかを見ていきましょう。\n","\n","まず、句読点・記号(\"・\", \"」\", \"「\", etc)や\n","助詞(の、に、は、を、が, etc)が高い頻度で現れていることがわかります。\n","\n","今回興味があるのは文書の内容であり、文構造ではないため、こういった語は無視するべきでしょう。\n","また同様に、数字や助動詞なども無視できるでしょう。\n","一方で、名詞・形容詞・動詞は内容に関連すると考えられるため、カウントすることにしましょう。"]},{"cell_type":"markdown","metadata":{"id":"_gWB-vBzNRfM"},"source":["それでは以下の点を変更して、もう一度やってみましょう:\n","\n","*   見出し語のみをカウントする\n","*   名詞・動詞・形容詞のみをカウントする\n","*   よくでてくるが今回の目的には関連しない語は、ブラックリストをつくって除外する"]},{"cell_type":"markdown","metadata":{"id":"s6FWoa0ViuzI"},"source":["## **第１実習問題：以下の点を変更して、もう一度やってみましょう:**\n","* 見出し語のみをカウントする\n","* 名詞・動詞・形容詞のみをカウントする\n","* よくでてくるが今回の目的には関連しない語は、ブラックリストを作って除外する\n"]},{"cell_type":"code","metadata":{"id":"VtWC-xC_7p4x"},"source":["def count_word_frequencies(text, words_blacklist, categories_whitelist, categories_blacklist):\n","  all_nouns_verbs_adjs = []\n","  tagger = MeCab.Tagger()\n","  for line in text:\n","      node = tagger.parseToNode(line)\n","      while(node):\n","        # ここにコードを書いてください\n","        node = node.next\n","        if node is None:\n","            break\n","  return Counter(all_nouns_verbs_adjs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--H1FzJycCQM"},"source":["# 無視したい単語\n","words_blacklist = [ ]\n","# 取得したい単語のカテゴリー\n","whitelist = [ ]\n","# 除外したいサブカテゴリー\n","blacklist = [ ]\n","with codecs.open(\"japanese_wikipedia_extracted_articles.txt\", \"r\",'utf-8') as text:\n","  all_nouns_verbs_adjs = count_word_frequencies(text, stopwords, whitelist, blacklist)\n","for word in all_nouns_verbs_adjs.most_common(25):\n","  print(\"Word: {}, frequency: {}\".format(word[0], word[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LLV9XJpxi636","cellView":"form"},"source":["#@title **[回答]** 上記の問題を書き終わったらこちらで回答を確認してください。\n","def count_word_frequencies(text, words_blacklist, categories_whitelist, categories_blacklist):\n","  all_nouns_verbs_adjs = []\n","  tagger = MeCab.Tagger()\n","  for line in text:\n","      node = tagger.parseToNode(line)\n","      while(node):\n","        lemma, pos, pos2 = get_word_lemma_and_pos_info(node)\n","        if lemma != '-' and lemma not in words_blacklist and pos in categories_whitelist and pos2 not in categories_blacklist:\n","            all_nouns_verbs_adjs.append(lemma)\n","        node = node.next\n","        if node is None:\n","            break\n","  return Counter(all_nouns_verbs_adjs)\n","\n","words_blacklist = ['する', 'なる', 'ない', 'これ', 'それ', 'id', 'ja', 'wiki',\n","             'wikipedia', 'id', 'doc', 'https', 'org', 'url', 'いう', 'ある',\n","             'curid', 'あれ', 'それら', 'これら', 'それそれ', 'それぞれ',\n","             'title', 'その後', '一部', '前', 'よる', '一つ', 'ひとつ', '他',\n","             'その他', 'ほか', 'そのほか', 'いる']\n","# 取得したい単語のカテゴリー\n","whitelist = ['名詞', '動詞', '形容詞']\n","# 除外したいサブカテゴリー\n","blacklist = ['非自立', '接尾', 'サ変接続', '数']\n","with codecs.open(\"japanese_wikipedia_extracted_articles.txt\", \"r\",'utf-8') as text:\n","  all_nouns_verbs_adjs = count_word_frequencies(text, words_blacklist, whitelist, blacklist)\n","for word in all_nouns_verbs_adjs.most_common(25):\n","  print(\"Word: {}, frequency: {}\".format(word[0], word[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQ0T0D0gfHDF"},"source":["# See what is the content of all_nouns_verbs_adjs\n","print([p for p in all_nouns_verbs_adjs.items()][:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTR_YTAa7h0B"},"source":["plot_word_frequency_distribution(all_nouns_verbs_adjs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSpDoxWwcW1V"},"source":["make_word_cloud(all_nouns_verbs_adjs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyvfjYYpOMPE"},"source":["今度はWikipediaの内容を表すような結果が得られました！"]},{"cell_type":"markdown","metadata":{"id":"kDYmZjUDqbPo"},"source":["### Wikipediaの記事で遊んでみる"]},{"cell_type":"markdown","metadata":{"id":"Pa6WV469OqZo"},"source":["せっかくなので全てのWikipediaの記事を対象に単語出現頻度を計算してみたいところですが、それにはこの講義だけでは時間が足りません。\n","そこで今回は、事前に単語出現頻度を日本語Wikipediaの全記事に対して計算し、使いやすいようにデータベースにしてきました。\n","\n","これを用いることで、それぞれの記事についての単語出現頻度を得て可視化ができます。\n","\n","(参考: \n","[データベースの作り方](https://colab.sandbox.google.com/drive/1C8QZlXQlAZKsRdfM950Zbp0s5eMR5ziG#scrollTo=m-Mtjjrx1Bwv))\n","\n","ではそのデータベースをダウンロードしてみましょう:"]},{"cell_type":"code","metadata":{"id":"KAjZweT4lTuW","cellView":"form"},"source":["#@title **[実行必要]** Wikipedia記事単位で単語出現頻度を計算済みデータベースをダウンロード\n","# Download the databse file with the preprocessed word frequencies.\n","!wget -O japanese_wikipedia_analysis.db https://storage.googleapis.com/nlp-lecture-data-20191029/japanese_wikipedia_analysis.db\n","DB_PATH = \"japanese_wikipedia_analysis.db\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l-Uk_y0_Qezg"},"source":["まずはあとで使うために2つの補助関数を用意しておきましょう。\n","\n","`retrieve_random_article` は指定した個数の記事をデータベースからランダムに取り出す関数、`retrieve_articles_wordfreqs_by_title` は指定したタイトルの記事をデータベースから取り出す関数です:"]},{"cell_type":"code","metadata":{"id":"u5ovTphknIqD","cellView":"form"},"source":["#@title **[実行必要]** データベースのインターフェースの定義：\n","#@markdown * retrieve_random_articles(db_path, amount)\n","def retrieve_random_articles(db_path, amount_articles):\n","  db_connection = sqlite3.connect(db_path)\n","  retrieve_statement = u\"\"\"\n","  SELECT title\n","  FROM article_text\n","  ORDER BY RANDOM() LIMIT \"\"\" + str(amount_articles)\n","  result = []\n","  cursor = db_connection.execute(retrieve_statement)\n","  for row in cursor:\n","      result.append(row[0])\n","  return result\n","\n","#@markdown * retrieve_articles_wordfreqs_by_title(db_path, article_title)\n","def retrieve_articles_wordfreqs_by_title(db_path, article_title):\n","  db_connection = sqlite3.connect(db_path)\n","  retrieve_statement = u\"\"\"\n","  SELECT\n","    word,\n","    frequency\n","  FROM article_word_frequencies\n","  INNER JOIN article_text\n","   ON article_text.article_id = article_word_frequencies.article_id\n","  WHERE title = \\\"{seq}\\\"\n","  \"\"\".format(seq=article_title)\n","  result = []\n","  cursor = db_connection.execute(retrieve_statement)\n","  for row in cursor:\n","      result.append(row)\n","  return sorted(result, key=lambda x: x[1])\n","\n","#@markdown * retrieve_wikipedia_wordfreqs(db_path)\n","def retrieve_wikipedia_wordfreqs(db_path):\n","  db_connection = sqlite3.connect(db_path)\n","  retrieve_statement = u\"\"\"\n","  SELECT\n","    word,\n","    frequency\n","  FROM wikipedia_word_frequencies\n","  \"\"\"\n","  result = []\n","  cursor = db_connection.execute(retrieve_statement)\n","  for row in cursor:\n","      result.append(row)\n","  return result\n","\n","#@markdown * retrieve_wikipedia_articles_amount(db_path)\n","def retrieve_wikipedia_articles_amount(db_path):\n","  db_connection = sqlite3.connect(db_path)\n","  retrieve_statement = u\"\"\"\n","  SELECT\n","    COUNT(DISTINCT(article_id))\n","  FROM article_text\n","  \"\"\"\n","  result = []\n","  cursor = db_connection.execute(retrieve_statement)\n","  for row in cursor:\n","      result.append(row)\n","  return result[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OacheAl_6pIc"},"source":["# retrieve 10 random articles:\n","print(retrieve_random_articles(DB_PATH, 10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcGpaWwHRJpk"},"source":["さあ、それでは定義した関数を使って、記事を１つ取り出して、ワードクラウドとして可視化してみましょう。"]},{"cell_type":"code","metadata":{"id":"PbtmDZ93DRh-"},"source":["title = '医学'\n","# 「医学」のウィキペディア記事の単語出現の分布\n","article_word_freqs = dict(retrieve_articles_wordfreqs_by_title(DB_PATH, title))\n","plot_word_frequency_distribution(article_word_freqs)\n","# ワードクラウドを作ってみましょう\n","print(title)\n","make_word_cloud(article_word_freqs)\n","# 他の記事で遊んでみてください。"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v7rHtepmsWT9"},"source":["### TF-IDF (https://en.wikipedia.org/wiki/Tf–idf)\n","TF-IDF (Term frequency-inverse document frequency)はドキュメント中の単語が、そのドキュメントにとってどの程度重要であるかを測る基本的な指標の一つです。\n","\n","TF-IDFは以下の２つの値の積によって得られます:\n","\n","- 対象の文章中の単語の出現頻度 (term frequency)\n","- 対象の単語が現れるドキュメントの割合 (document frequency) を、反転したもの (inverse)\n","\n","TF-IDFは単語がドキュメント中にどの程度頻繁にあられるかという指標 (TF) と、その単語がどの程度その文書に特有のものであるか (IDF) の２つを考慮します。例えば、前置詞・代名詞などの全ての文書にあらわれるような頻出語は文章中に何度もあらわれ (TFが高い) としてもIDFが小さくなるのでTF-IDFのスコアは低くなります。\n","\n","一方で、\"自然言語処理\"のようなあまり多くの文書に出現しない単語は、IDFが大きくなるため、その単語があらわれる文書におけるTF-IDFは高くなり、その文書において「関連性の高い」単語だと評価されます。\n","\n","TF-IDFは文書検索・文書分析や機械学習アルゴリズムに与える文書のスコアなどとして広く利用されています。\n","\n","## TF-IDFの計算方法\n","\n","上述したようにTF-IDFは以下の２つの指標の積を計算することで得られます\n","\n","**TF (The term frequency of a word in a document):**\n","\n","[TFを計算する方法にはいくつかのバリエーションがあります](https://en.wikipedia.org/wiki/Tf–idf#Definition)。単に単語の出現回数をTFとすることもできますし、ドキュメントの単語数で割って単語の出現割合をTFとすることもできます。\n","\n","今回は出現回数$f$に1を足して$\\log$を計算したものをTFとして利用します。\n","\n","**IDF (The inverse document frequency of the word across a the set of all the other documents):**\n","\n","IDFはあたえられた文書のデータセット全体の中で、ある単語がどの程度頻繁に・あるいは稀に出現するかを表す指標です。頻出ほど低く、稀な単語ほど高いスコアになるように設計します。\n","\n","[IDFの計算方法にもいくつかのバリエーションが存在します](https://en.wikipedia.org/wiki/Tf–idf#Definition)。今回は全ての文書の数を、その単語があらわれる文書の数で割ってlogをとったものをIDFとして使用します。\n","\n","$$\\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$$\n","\n","このIDFは全てのドキュメントにあらわれる単語に対して$0$になり、稀な単語ほど対数スケールで数が大きくなります。\n","\n","まとめると、この授業で使うTF-IDFは以下の定義となります:\n","(t: 単語, d: 文書, N: 文書の総数, D: 文書の集合)\n","\n","```\n","    tf_idf(t, d, D) = tf(t, d) * idf(t, D)\n","```\n","\n","```\n","    tf(t, d) = log(1 + freq(t, d))\n","```\n","```\n","    idf(t, D) = log(N / count(d in D: t in d))\n","```\n","\n","### TF-IDFの実用例\n","\n","TF-IDFは自然言語処理における基本的かつ重要な技術のひとつであり、例えば以下のような用途で使用することができます。\n","\n","**情報検索**\n","\n","検索している入力と文書の関連度をTF-IDFを用いて計算し、関連度の高い文書を探してくることで、検索エンジンのような文書検索を行うことができます。\n","\n","**キーワード抽出**\n","\n","他のTF-IDF適用例として文書からのキーワード抽出があげられます。文書中のTF-IDFの高い単語を抽出することで、その文書にとって重要度が高いキーワードを抽出することができます。\n","\n","実際にTF-IDFを計算するコードを書いてみましょう。"]},{"cell_type":"markdown","metadata":{"id":"APF4VXLBmnto"},"source":["## **第２実習問題：紹介されたTF-IDFの関数を定義しましょう:**"]},{"cell_type":"code","metadata":{"id":"WNrKYFQPmW6n"},"source":["def tf_idf(word, doc_word_frequencies, corpus_word_frequencies, dataset_size):\n","  pass\n","\n","def tf(word, doc_word_frequencies):\n","  pass\n","\n","def idf(word, corpus_word_frequencies, amount_of_documents):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFepQPvR616h","cellView":"form"},"source":["#@title **[回答]** 上記の問題を書き終わったらこちらで回答を確認してください。\n","from math import log\n","def tf_idf(word, doc_word_frequencies, corpus_word_frequencies, dataset_size):\n","  return tf(word, doc_word_frequencies) * idf(word, corpus_word_frequencies, dataset_size)\n","\n","def tf(word, doc_word_frequencies):\n","  return log(1 + doc_word_frequencies[word])\n","\n","def idf(word, corpus_word_frequencies, amount_of_documents):\n","  # to avoid dividing by 0\n","  if word not in corpus_word_frequencies or corpus_word_frequencies[word] == 0:\n","    return 1\n","  return  log(amount_of_documents / corpus_word_frequencies[word])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mw67V-5hrXFN"},"source":["単語に対するTF-IDFを計算する関数が定義できたので、それを使って1つの文書にあらわれる全ての単語に対するTF-IDFを計算する関数も定義します"]},{"cell_type":"code","metadata":{"id":"OnPzpS6G65iD"},"source":["wikipedia_frequencies = retrieve_wikipedia_wordfreqs(DB_PATH)\n","wikipedia_size = retrieve_wikipedia_articles_amount(DB_PATH)\n","\n","def calculate_articles_tfidfs(db_path, title):\n","  article_word_frequencies = dict(retrieve_articles_wordfreqs_by_title(db_path, title))\n","  tfidfs_dict = {}\n","  for word in article_word_frequencies:\n","    tfidfs_dict[word] = round(tf_idf(word, article_word_frequencies, wikipedia_frequencies, wikipedia_size), 10)\n","  return tfidfs_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2VnXS4KqqnO"},"source":["先ほど行った可視化を今度はTF-IDFを使って行ってみましょう。"]},{"cell_type":"code","metadata":{"id":"KK1fs_bO3Yf8"},"source":["# 同じ「医学」という記事のTF-IDFに基づいたワードクラウドを作ってみましょう。\n","title = '医学'\n","# 単語のTF-IDF値の分布をみてみましょう。\n","article_words_tfidfs = calculate_articles_tfidfs(DB_PATH, title)\n","plot_word_frequency_distribution(article_words_tfidfs)\n","print(title)\n","make_word_cloud(article_words_tfidfs)\n","# 他の記事でも遊んでみてください。"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SX_TE5Ro3b_E"},"source":["# Compare tfidf scores between two similar articles\n","tfidf1 = sorted(dict(calculate_articles_tfidfs(DB_PATH, \"薬学\")).items(), key=lambda x: x[1], reverse=True)\n","tfidf2 = sorted(dict(calculate_articles_tfidfs(DB_PATH, \"医学\")).items(), key=lambda x: x[1], reverse=True)\n","# Let's compare their top tfidf scoring words\n","print(\"These are the top scoring tfidf words for both articles:\")\n","print(\"\\tタイトル：薬学\\t|\\tタイトル：医学\")\n","for index in range(25):\n","  print(\"{}: {}\\t|\\t{}\".format(index, tfidf1[index], tfidf2[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mF4ri9N93dHI"},"source":["# Compare tfidf scores between two distant articles\n","tfidf1 = sorted(dict(calculate_articles_tfidfs(DB_PATH, \"薬学\")).items(), key=lambda x: x[1], reverse=True)\n","tfidf2 = sorted(dict(calculate_articles_tfidfs(DB_PATH, \"ピアノ\")).items(), key=lambda x: x[1], reverse=True)\n","# Let's compare their top tfidf scoring words\n","print(\"These are the top scoring tfidf words for both articles:\")\n","print(\"\\tタイトル：薬学\\t|\\tタイトル：ピアノ\")\n","for index in range(25):\n","  print(\"{}: {}\\t|\\t{}\".format(index, tfidf1[index], tfidf2[index]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aDlWXtNZqQap"},"source":["### TF-IDF と機械学習\n","\n","機械学習のアルゴリズムは数値演算を行う関数を学習したい対象に合わせて最適化することで動作します。しかし自然言語は文字列なのでそのままでは数値として扱うことができず、機械学習のアルゴリズムを適用することが困難です。そのためまず自然言語を数値のベクトルによる表現に変換する必要があります。\n","\n","自然言語を数値のベクトルに変換する方法は色々と存在し(TF-IDF bag of words, word embedding など)、どのアルゴリズムを使用するかは最終的に機械学習で得られるモデルのパフォーマンスに大きく影響します。\n","\n","ここでは先ほど紹介したTF-IDFを使って文書を数値のベクトルに変換してみます。例えば、データセット中の文章に現れる全ての単語が:\n","\n","```\n","vocab = {tree, dog, day, cat, climb, one, sunny, the, walk, quickly}\n","```\n","\n","であるとき、document_1 における各単語のTF-IDFが以下のようになったとします(0は単語が出現しないことを意味します):\n","\n","```\n","document_1 = {tree: 0.65, dog: 0, day: 0, cat: 0.81, climb: 0.9, one: 0, sunny:0, the: 0.02, walk: 0, quickly: 0.34}\n","```\n","\n","これを単語の順序を固定して配列に変換すると数値ベクトルが得られます。\n","\n","```\n","document_1_vector = [0.65, 0, 0, 0.81, 0.9, 0, 0, 0.02, 0, 0.34]\n","```\n","\n","これは文書document_1を数値ベクトルで表現したものとして使用することができます。\n","このようにして文書を数値のベクトルに変換すると、文書をアルゴリズムで取り扱いやすくなります。実際に、このベクトル表現を使って文書間の「類似度」を計算してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"nfWz5bNJXVTq"},"source":["## 類似度 (Similarity) の計算\n","\n","上に述べたような方法で文章を数値のベクトルとして表現することができれば、2つの文章間の距離を計算することができるようになります。\n","\n","NLPで広く使われる類似度の１つがコサイン距離を使ったコサイン類似度 (Cosine Similarity)です。コサイン類似度は２つの文書のベクトルの角度のコサインを計算することで得られます\n","\n","A, B を文書A, 文書B のベクトル表現とすると、コサイン類似度は:\n","\n","![Cosine Similarity](https://miro.medium.com/max/852/1*hub04IikybZIBkSEcEOtGA.png)\n","\n","で計算できます。\n","\n","２つの文書のコサイン類似度を計算する計算するには、まず文書を数値のベクトル表現に変換する必要があります。ここでは先ほど紹介したTF-IDFを利用したベクトル表現を使用します。"]},{"cell_type":"markdown","metadata":{"id":"oJZNMi7f93pz"},"source":["上記の計算式でベクトル間の距離を計算する関数を定義しましょう"]},{"cell_type":"markdown","metadata":{"id":"Bv0_CHW6oduF"},"source":["## **第３実習問題：紹介されたコサイン類似度の関数を定義しましょう:**"]},{"cell_type":"code","metadata":{"id":"hMLmVycdokB0"},"source":["# ベクターのコサイン類似度の計算する関数\n","# article1とarticle2のフォマットは{word: tfidf_score}\n","def similarity(article1, article2):\n","   pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGpgPo9qXoG2","cellView":"form"},"source":["#@title **[回答]** 上記の問題を書き終わったらこちらで回答を確認してください。\n","from math import sqrt\n","\n","# ベクターのコサイン類似度の計算する関数\n","# article1とarticle2のフォマットは{word: tfidf_score}\n","def similarity(article1, article2):\n","  numerator_sum = 0\n","  # 共通の単語のみの足し算を計算する\n","  for word1 in article1:\n","    for word2 in article2:\n","      if word1 == word2:\n","        numerator_sum += article1[word1]*article2[word2]\n","  # 各ベクターの全ての単語の二乗したTF-IDF値を足し算する\n","  article1_squared_sum = 0\n","  for word in article1:\n","    article1_squared_sum += article1[word]**2\n","  article2_squared_sum = 0\n","  for word in article2:\n","    article2_squared_sum += article2[word]**2\n","  return numerator_sum / (sqrt(article1_squared_sum)*sqrt(article2_squared_sum))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLsOs_9VI6qr"},"source":["「類似度」を実際の例で計算してみましょう。例の中でもっとも類似度が高そうに直感的に思う文章の類似度が実際に大きくなることを確かめてみます。"]},{"cell_type":"code","metadata":{"id":"wP84RwZRYn3_"},"source":["# calculate the similarity between 5 documents\n","def print_similarity_scores(target_article, articles_list):\n","  article1 = dict(calculate_articles_tfidfs(DB_PATH, target_article))\n","  for article_title in articles_list:\n","    article2 = dict(calculate_articles_tfidfs(DB_PATH, article_title))\n","    print(\"Similarity score between {} and {} is {}\".format(target_article, article_title, similarity(article1, article2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0LMHF4a5T_-"},"source":["print_similarity_scores(\"薬学\", [\"ピアノ\", \"医学\", \"哲学\", \"物理学\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Ob3DlCP5VPs"},"source":["print_similarity_scores(\"アルゼンチン\", [\"チリ\", \"言語\", \"IBM\", \"亜鉛\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-tfUd-2Gybi"},"source":["こういうふうに類似度の計算を通して、与えられたウィキペディア記事、あるいは与えられた検索クエリと最も関連しているウィキペディアの記事を抽出することができます。\n","\n","おめでとうございます！このノートブックでは:\n","\n","* NLPにおけるテキスト処理の基本\n","* Wikipediaのような巨大なデータセットをどのようにしてロードし、文章中のもっとも関連性の高いを抽出するか\n","* 単語頻度の分布のグラフの描画方法\n","* 文書中の単語をワードクラウドのような目を引くフォーマットで可視化する方法\n","* IF-IDFを使って二つの文章間の「類似性」を計算する方法\n","\n","などを学びました。\n","\n","このような基礎的な情報抽出の手段も含め、さまざまなNLPの手法が実際の検索エンジンでも利用されています。"]}]}